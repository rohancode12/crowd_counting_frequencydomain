{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10fc48e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (32, 512, 512, 3)\n",
      "Characteristic function shape: (32, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "\n",
    "class CrowdData(tf.data.Dataset):\n",
    "    def __new__(cls, img_path, dot_ann_path, mode='train', is_gray=False, min_size=0, max_size=np.inf, target_size=(512, 512)):\n",
    "        img_list = sorted(glob(os.path.join(img_path, '*.jpg')))\n",
    "        dot_ann_list = sorted(glob(os.path.join(dot_ann_path, '*.mat')))\n",
    "        \n",
    "        if len(img_list) == 0:\n",
    "            raise ValueError(f\"No .jpg files found in directory {img_path}\")\n",
    "        if len(dot_ann_list) == 0:\n",
    "            raise ValueError(f\"No .mat files found in directory {dot_ann_path}\")\n",
    "        if len(img_list) != len(dot_ann_list):\n",
    "            raise ValueError(f\"Mismatch in number of images ({len(img_list)}) and annotations ({len(dot_ann_list)})\")\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((img_list, dot_ann_list))\n",
    "        dataset = dataset.map(lambda x, y: cls._process_data(x, y, is_gray, min_size, max_size, mode, target_size))\n",
    "        \n",
    "        if mode == 'train':\n",
    "            dataset = dataset.shuffle(len(img_list)).batch(32)  # adjust batch size as needed\n",
    "        else:\n",
    "            dataset = dataset.batch(32)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_data(img_path, dot_ann_path, is_gray, min_size, max_size, mode, target_size):\n",
    "        def load_and_preprocess(img_path, dot_ann_path):\n",
    "            img_path = tf.compat.as_str(img_path.numpy())\n",
    "            dot_ann_path = tf.compat.as_str(dot_ann_path.numpy())\n",
    "            \n",
    "            img = Image.open(img_path)\n",
    "            if is_gray:\n",
    "                img = img.convert('L')\n",
    "            else:\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            w, h = img.size\n",
    "            if min([w, h]) < min_size:\n",
    "                r = min_size / min([w, h])\n",
    "                img = img.resize((int(w * r), int(h * r)))\n",
    "            elif min([w, h]) > max_size:\n",
    "                r = max_size / min([w, h])\n",
    "                img = img.resize((int(w * r), int(h * r)))\n",
    "            \n",
    "            img = np.array(img, dtype=np.float32) / 255.0\n",
    "            img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "            \n",
    "            gt_data = loadmat(dot_ann_path)['image_info'][0][0][0][0][0]\n",
    "            dot_ann = gt_data[:, :2] if gt_data.shape[0] > 0 else np.zeros((0, 2), dtype=np.float32)\n",
    "            \n",
    "            return img, dot_ann\n",
    "\n",
    "        img, dot_ann = tf.py_function(\n",
    "            func=load_and_preprocess,\n",
    "            inp=[img_path, dot_ann_path],\n",
    "            Tout=[tf.float32, tf.float32]\n",
    "        )\n",
    "        \n",
    "        # Set an initial shape for img to allow resizing\n",
    "        img.set_shape([None, None, 3])\n",
    "\n",
    "        # Resize image to target size for consistent batching\n",
    "        img = tf.image.resize(img, target_size)\n",
    "        img.set_shape([*target_size, 3])  # Explicit shape for batching\n",
    "        dot_ann.set_shape([None, 2])      # Variable shape for dot annotations\n",
    "\n",
    "        if mode == 'train':\n",
    "            chf = tf.zeros([64, 64], dtype=tf.float32)\n",
    "            return img, chf\n",
    "        else:\n",
    "            return img, dot_ann, tf.shape(dot_ann)[0], img_path\n",
    "\n",
    "# Instantiate the dataset\n",
    "img_path = 'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\images'\n",
    "dot_ann_path = 'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\ground_truth'\n",
    "train_dataset = CrowdData(img_path=img_path, dot_ann_path=dot_ann_path, mode='train', is_gray=False, min_size=256, max_size=1024, target_size=(512, 512))\n",
    "\n",
    "for img, chf in train_dataset.take(1):\n",
    "    print(\"Image shape:\", img.shape)\n",
    "    print(\"Characteristic function shape:\", chf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f12b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "class ImgTensorDotTensorProcessing:\n",
    "    @staticmethod\n",
    "    def crop(img_tensor, dot_tensor, crop_position):\n",
    "        '''\n",
    "        Crop image and dot tensors.\n",
    "\n",
    "        Args:\n",
    "            img_tensor: Image tensor (H, W, C)\n",
    "            dot_tensor: Tensor containing coordinates of dots\n",
    "            crop_position: Tuple of four integers (left, upper, right, lower)\n",
    "\n",
    "        Returns: Cropped image tensor and adjusted dot tensor\n",
    "        '''\n",
    "        assert 0 <= crop_position[0] < crop_position[2] <= img_tensor.shape[1] and \\\n",
    "               0 <= crop_position[1] < crop_position[3] <= img_tensor.shape[0]\n",
    "\n",
    "        img_tensor = img_tensor[crop_position[1]:crop_position[3], crop_position[0]:crop_position[2], :]\n",
    "\n",
    "        if dot_tensor.shape[0] > 0:\n",
    "            mask = (dot_tensor[:, 0] > crop_position[0]) & (dot_tensor[:, 0] < crop_position[2]) & \\\n",
    "                   (dot_tensor[:, 1] > crop_position[1]) & (dot_tensor[:, 1] < crop_position[3])\n",
    "            dot_tensor = dot_tensor[mask]\n",
    "            dot_tensor = dot_tensor - tf.constant(crop_position[0:2], dtype=dot_tensor.dtype)\n",
    "\n",
    "        return img_tensor, dot_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def random_crop(img_tensor, dot_tensor, size, crop_mode):\n",
    "        '''\n",
    "        Random crop an image tensor and dot tensor.\n",
    "        \n",
    "        Args:\n",
    "            img_tensor: Image tensor (H, W, C)\n",
    "            dot_tensor: Tensor containing coordinates of dots\n",
    "            size: Crop size, single value or (w, h)\n",
    "            crop_mode: Crop function to use\n",
    "\n",
    "        Returns: Cropped image and dot tensor\n",
    "        '''\n",
    "        size = np.array(size)\n",
    "        selectable_range = np.array([img_tensor.shape[1], img_tensor.shape[0]]) - size\n",
    "        assert (selectable_range >= 0).all()\n",
    "\n",
    "        left_up = (np.random.rand(2) * selectable_range).astype(int)\n",
    "        right_down = (left_up + size).astype(int)\n",
    "        return crop_mode(img_tensor, dot_tensor, (left_up[0], left_up[1], right_down[0], right_down[1]))\n",
    "\n",
    "    @staticmethod\n",
    "    def random_mirror(img_tensor, dot_tensor):\n",
    "        '''\n",
    "        Randomly mirror the image tensor.\n",
    "\n",
    "        Args:\n",
    "            img_tensor: Image tensor (H, W, C)\n",
    "            dot_tensor: Tensor containing coordinates of dots\n",
    "\n",
    "        Returns: Mirrored image tensor and dot tensor\n",
    "        '''\n",
    "        if np.random.rand() > 0.5:\n",
    "            img_tensor = tf.image.flip_left_right(img_tensor)\n",
    "            if dot_tensor.shape[0] > 0:\n",
    "                dot_tensor = tf.concat([img_tensor.shape[1] - dot_tensor[:, 0:1], dot_tensor[:, 1:2]], axis=1)\n",
    "        return img_tensor, dot_tensor\n",
    "\n",
    "\n",
    "class ImageDotmapProcessing:\n",
    "    @staticmethod\n",
    "    def crop(img, dotted_map, crop_position=(0, 0, 512, 512)):\n",
    "        '''\n",
    "        Crop image and dotted map.\n",
    "\n",
    "        Args:\n",
    "            img: PIL Image\n",
    "            dotted_map: Array of dot coordinates\n",
    "            crop_position: Tuple (left, upper, right, lower)\n",
    "\n",
    "        Returns: Cropped image and dotted map\n",
    "        '''\n",
    "        img = img.crop(crop_position)\n",
    "        if dotted_map.shape[0] > 0:\n",
    "            mask = (dotted_map[:, 0] > crop_position[0]) & (dotted_map[:, 0] < crop_position[2]) & \\\n",
    "                   (dotted_map[:, 1] > crop_position[1]) & (dotted_map[:, 1] < crop_position[3])\n",
    "            dotted_map = dotted_map[mask]\n",
    "            dotted_map[:, 0:2] -= np.array(crop_position[0:2])\n",
    "\n",
    "        return img, dotted_map\n",
    "\n",
    "    @staticmethod\n",
    "    def random_crop(img, dotted_map, size):\n",
    "        '''\n",
    "        Randomly crop image and dotted map.\n",
    "\n",
    "        Args:\n",
    "            img: PIL Image\n",
    "            dotted_map: Array of dot coordinates\n",
    "            size: Crop size, single value or (w, h)\n",
    "\n",
    "        Returns: Cropped image and dotted map\n",
    "        '''\n",
    "        size = np.array(size)\n",
    "        selectable_range = np.array(img.size) - size\n",
    "        assert (selectable_range >= 0).all()\n",
    "\n",
    "        left_up = np.random.rand(2) * selectable_range\n",
    "        right_down = left_up + size\n",
    "        return ImageDotmapProcessing.crop(img, dotted_map, tuple(np.concatenate((left_up, right_down))))\n",
    "\n",
    "    @staticmethod\n",
    "    def resize(img, dotted_map, size=512):\n",
    "        '''\n",
    "        Resize image and dotted map.\n",
    "\n",
    "        Args:\n",
    "            img: PIL Image\n",
    "            dotted_map: Array of dot coordinates\n",
    "            size: Resize dimensions, single value or (w, h)\n",
    "\n",
    "        Returns: Resized image and dotted map\n",
    "        '''\n",
    "        size = np.array(size)\n",
    "        ratio = size / np.array(img.size)\n",
    "        img = img.resize(size)\n",
    "\n",
    "        if dotted_map.shape[0] > 0:\n",
    "            dotted_map *= ratio\n",
    "\n",
    "        return img, dotted_map\n",
    "\n",
    "    @staticmethod\n",
    "    def random_mirror(img, dotted_map):\n",
    "        '''\n",
    "        Randomly mirror image and dotted map.\n",
    "\n",
    "        Args:\n",
    "            img: PIL Image\n",
    "            dotted_map: Array of dot coordinates\n",
    "\n",
    "        Returns: Mirrored image and dotted map\n",
    "        '''\n",
    "        w, h = img.size\n",
    "        if np.random.rand() > 0.5:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            if dotted_map.shape[0] > 0:\n",
    "                dotted_map[:, 0] = w - dotted_map[:, 0]\n",
    "        return img, dotted_map\n",
    "\n",
    "\n",
    "class GeneratingDataFromDottedAnnotation:\n",
    "    @staticmethod\n",
    "    def construct_characteristic_function(head_position, bandwidth, origin=0,\n",
    "                                          step=30, step_length=0.01):\n",
    "        '''\n",
    "        Construct discretized characteristic function.\n",
    "\n",
    "        Args:\n",
    "            head_position: 2D tensor with head coordinates\n",
    "            bandwidth: Bandwidth of Gaussian\n",
    "            origin: Origin of the image plane\n",
    "            step: Steps the function spans in each direction\n",
    "            step_length: Span of each step\n",
    "\n",
    "        Returns: Characteristic function tensor\n",
    "        '''\n",
    "        if head_position.shape[0] > 0:\n",
    "            gauss_mean = head_position - origin\n",
    "            bandwidth = tf.reshape(bandwidth, (1, 1, head_position.shape[0])) if not isinstance(bandwidth, (int, float)) else bandwidth\n",
    "\n",
    "            grid_x, grid_y = tf.meshgrid(tf.range(-step, step, dtype=tf.float32),\n",
    "                                         tf.range(-step, step, dtype=tf.float32))\n",
    "            plane = tf.stack([grid_x * step_length, grid_y * step_length], axis=2)\n",
    "\n",
    "            angle = tf.einsum('ijk,lk->ijl', plane, gauss_mean)\n",
    "            length = tf.exp(-0.5 * tf.reduce_sum(tf.square(plane), axis=2, keepdims=True) * bandwidth ** 2)\n",
    "            angle_real = tf.cos(angle)\n",
    "            angle_img = tf.sin(angle)\n",
    "\n",
    "            cf_real = tf.reduce_sum(angle_real * length, axis=2, keepdims=True)\n",
    "            cf_img = tf.reduce_sum(angle_img * length, axis=2, keepdims=True)\n",
    "            return tf.concat([cf_real, cf_img], axis=2)\n",
    "        else:\n",
    "            return tf.zeros((step * 2, step * 2, 2), dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d96524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "\n",
    "class TransGtToNdarray:\n",
    "    @staticmethod\n",
    "    def trans_ann_to_npy_SHTC(target_path: str, save_path: str):\n",
    "        \"\"\"\n",
    "        Converts ShanghaiTech Part A `.mat` annotation files to `.npy` format.\n",
    "        \n",
    "        Args:\n",
    "            target_path (str): Directory containing the original `.mat` annotation files.\n",
    "            save_path (str): Directory where the converted `.npy` annotation files will be saved.\n",
    "        \"\"\"\n",
    "        assert target_path != save_path, \"Target and save paths must be different.\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "            \n",
    "        for file in glob.glob(target_path + '/*.mat'):\n",
    "            dot = loadmat(file)\n",
    "            x = dot[list(dot.keys())[-1]][0, 0]['location'][0, 0].astype(np.float32)\n",
    "            np.save(os.path.join(save_path, os.path.basename(file).split('.')[0] + '.npy'), x)\n",
    "\n",
    "class DirectoryPath:\n",
    "    @staticmethod\n",
    "    def prefix_suffix(dataset_name: str):\n",
    "        if 'shanghaitech' in dataset_name.lower() and 'a' in dataset_name.lower():\n",
    "            return 'Dataset/ShanghaiTech/part_A_final/', '_data/images', '_data/ground_truth_npy'\n",
    "\n",
    "    @staticmethod\n",
    "    def get_name_from_no(dataset_name: str, set_type: str, prefix: str, img_suffix: str, dotmap_suffix: str, img_no):\n",
    "        \"\"\"\n",
    "        Generates file paths for images and annotation files based on image number.\n",
    "\n",
    "        Args:\n",
    "            dataset_name (str): Name of the dataset.\n",
    "            set_type (str): Data subset type ('train' or 'test').\n",
    "            prefix (str): Dataset base directory.\n",
    "            img_suffix (str): Image directory suffix.\n",
    "            dotmap_suffix (str): Annotation directory suffix.\n",
    "            img_no (int or str): Image number.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Image path and dot map path.\n",
    "        \"\"\"\n",
    "        if isinstance(img_no, int):\n",
    "            img_path = os.path.join(prefix, set_type, img_suffix, f'IMG_{img_no}.jpg')\n",
    "            dotmap_path = os.path.join(prefix, set_type, dotmap_suffix, f'GT_IMG_{img_no}.npy')\n",
    "        elif isinstance(img_no, str):\n",
    "            img_path = os.path.join(prefix, set_type, img_suffix, f'{img_no}.jpg')\n",
    "            dotmap_path = os.path.join(prefix, set_type, dotmap_suffix, f'GT_{img_no}.npy')\n",
    "        else:\n",
    "            raise ValueError(\"img_no must be an int or str.\")\n",
    "        return img_path, dotmap_path\n",
    "\n",
    "class DatasetPreparation:\n",
    "    @staticmethod\n",
    "    def SHTCA():\n",
    "        \"\"\"\n",
    "        Prepares the ShanghaiTech Part A dataset by converting `.mat` files to `.npy` format.\n",
    "        \"\"\"\n",
    "        TransGtToNdarray.trans_ann_to_npy_SHTC(\n",
    "            'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\ground_truth',\n",
    "            'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\1'\n",
    "        )\n",
    "        TransGtToNdarray.trans_ann_to_npy_SHTC(\n",
    "            'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\test_data\\\\ground_truth',\n",
    "            'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\test_data\\\\11'\n",
    "        )\n",
    "\n",
    "class ImageDotmapProcessing:\n",
    "    @staticmethod\n",
    "    def resize(img, dotted_map, size=(512, 512)):\n",
    "        \"\"\"\n",
    "        Resizes image and adjusts dot annotations.\n",
    "\n",
    "        Args:\n",
    "            img (PIL.Image): PIL image.\n",
    "            dotted_map (numpy.ndarray): Annotation points.\n",
    "            size (tuple): Target size (width, height).\n",
    "\n",
    "        Returns:\n",
    "            tuple: Resized image and adjusted annotations.\n",
    "        \"\"\"\n",
    "        size = np.array(size)\n",
    "        ratio = size / np.array(img.size)\n",
    "        img = img.resize(size)\n",
    "        \n",
    "        if dotted_map.shape[0] > 0:\n",
    "            dotted_map = dotted_map * ratio\n",
    "        \n",
    "        return img, dotted_map\n",
    "\n",
    "class BatchImageDotmapProcessing:\n",
    "    @staticmethod\n",
    "    def resize(dataset_name: str, set_type: str, min_side_length: int, max_side_length: int, dotmap_together=True, is_gray=False):\n",
    "        \"\"\"\n",
    "        Batch processes images and dot maps in the dataset by resizing them.\n",
    "\n",
    "        Args:\n",
    "            dataset_name (str): Dataset name.\n",
    "            set_type (str): Data subset type ('train' or 'test').\n",
    "            min_side_length (int): Minimum side length for resizing.\n",
    "            max_side_length (int): Maximum side length for resizing.\n",
    "            dotmap_together (bool): Whether to resize the dot map together with the image.\n",
    "            is_gray (bool): Whether to load images as grayscale.\n",
    "        \"\"\"\n",
    "        def resize_img(img_path, dotmap_path, dotmap_together, is_gray):\n",
    "            img = Image.open(img_path).convert('L') if is_gray else Image.open(img_path).convert('RGB')\n",
    "            dot_ann = np.load(dotmap_path) if dotmap_together else None\n",
    "\n",
    "            w, h = img.size\n",
    "            new_h, new_w, ratio = cal_new_size(h, w, min_side_length, max_side_length)\n",
    "            \n",
    "            if ratio != 1:\n",
    "                img = img.resize((new_w, new_h))\n",
    "                img.save(img_path, quality=95)\n",
    "                if dotmap_together and dot_ann.shape[0] > 0:\n",
    "                    dot_ann = dot_ann * ratio\n",
    "                    np.save(dotmap_path, dot_ann)\n",
    "                print(f\"Processed: {img_path}, {dotmap_path}\")\n",
    "\n",
    "        prefix, img_suffix, dotmap_suffix = DirectoryPath.prefix_suffix(dataset_name)\n",
    "        for img_path in glob.glob(os.path.join(prefix, set_type, img_suffix, '*.jpg')):\n",
    "            num = DirectoryPath.get_name_from_no(dataset_name, set_type, prefix, img_suffix, dotmap_suffix, os.path.basename(img_path).split('_')[-1].split('.')[0])\n",
    "            resize_img(img_path, num[1], dotmap_together, is_gray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f5e081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db01abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "\n",
    "class TransGtToNdarray:\n",
    "    @staticmethod\n",
    "    def trans_ann_to_npy_SHTC(target_path: str, save_path: str):\n",
    "        \"\"\"\n",
    "        Converts ShanghaiTech Part A `.mat` annotation files to `.npy` format.\n",
    "        \n",
    "        Args:\n",
    "            target_path (str): Directory containing the original `.mat` annotation files.\n",
    "            save_path (str): Directory where the converted `.npy` annotation files will be saved.\n",
    "        \"\"\"\n",
    "        assert target_path != save_path, \"Target and save paths must be different.\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        for file in glob.glob(os.path.join(target_path, '*.mat')):\n",
    "            dot = loadmat(file)\n",
    "            # Extract 'location' key for annotations\n",
    "            x = dot[list(dot.keys())[-1]][0, 0]['location'][0, 0].astype(np.float32)\n",
    "            npy_file_path = os.path.join(save_path, os.path.basename(file).split('.')[0] + '.npy')\n",
    "            np.save(npy_file_path, x)\n",
    "            print(f\"Saved annotation to: {npy_file_path}\")\n",
    "\n",
    "class DirectoryPath:\n",
    "    @staticmethod\n",
    "    def prefix_suffix(dataset_name: str):\n",
    "        if 'shanghaitech' in dataset_name.lower() and 'a' in dataset_name.lower():\n",
    "            return 'Dataset/ShanghaiTech/part_A_final/', '_data/images', '_data/ground_truth_npy'\n",
    "\n",
    "    @staticmethod\n",
    "    def get_name_from_no(dataset_name: str, set_type: str, prefix: str, img_suffix: str, dotmap_suffix: str, img_no):\n",
    "        \"\"\"\n",
    "        Generates file paths for images and annotation files based on image number.\n",
    "\n",
    "        Args:\n",
    "            dataset_name (str): Name of the dataset.\n",
    "            set_type (str): Data subset type ('train' or 'test').\n",
    "            prefix (str): Dataset base directory.\n",
    "            img_suffix (str): Image directory suffix.\n",
    "            dotmap_suffix (str): Annotation directory suffix.\n",
    "            img_no (int or str): Image number.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Image path and dot map path.\n",
    "        \"\"\"\n",
    "        if isinstance(img_no, int):\n",
    "            img_path = os.path.join(prefix, set_type, img_suffix, f'IMG_{img_no}.jpg')\n",
    "            dotmap_path = os.path.join(prefix, set_type, dotmap_suffix, f'GT_IMG_{img_no}.npy')\n",
    "        elif isinstance(img_no, str):\n",
    "            img_path = os.path.join(prefix, set_type, img_suffix, f'{img_no}.jpg')\n",
    "            dotmap_path = os.path.join(prefix, set_type, dotmap_suffix, f'GT_{img_no}.npy')\n",
    "        else:\n",
    "            raise ValueError(\"img_no must be an int or str.\")\n",
    "        return img_path, dotmap_path\n",
    "\n",
    "class DatasetPreparation:\n",
    "    @staticmethod\n",
    "    def SHTCA():\n",
    "        \"\"\"\n",
    "        Prepares the ShanghaiTech Part A dataset by converting `.mat` files to `.npy` format.\n",
    "        \"\"\"\n",
    "        TransGtToNdarray.trans_ann_to_npy_SHTC(\n",
    "            'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\ground_truth',\n",
    "            'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\ground_truth_npy'\n",
    "        )\n",
    "        TransGtToNdarray.trans_ann_to_npy_SHTC(\n",
    "            'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\test_data\\\\ground_truth',\n",
    "            'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\test_data\\\\ground_truth_npy'\n",
    "        )\n",
    "\n",
    "class ImageDotmapProcessing:\n",
    "    @staticmethod\n",
    "    def resize(img, dotted_map, size=(512, 512)):\n",
    "        \"\"\"\n",
    "        Resizes image and adjusts dot annotations.\n",
    "\n",
    "        Args:\n",
    "            img (PIL.Image): PIL image.\n",
    "            dotted_map (numpy.ndarray): Annotation points.\n",
    "            size (tuple): Target size (width, height).\n",
    "\n",
    "        Returns:\n",
    "            tuple: Resized image and adjusted annotations.\n",
    "        \"\"\"\n",
    "        size = np.array(size)\n",
    "        ratio = size / np.array(img.size)\n",
    "        img = img.resize(size)\n",
    "        \n",
    "        if dotted_map.shape[0] > 0:\n",
    "            dotted_map = dotted_map * ratio\n",
    "        \n",
    "        return img, dotted_map\n",
    "\n",
    "class BatchImageDotmapProcessing:\n",
    "    @staticmethod\n",
    "    def cal_new_size(h, w, min_side_length, max_side_length):\n",
    "        \"\"\"\n",
    "        Calculates new size for an image while preserving the aspect ratio.\n",
    "\n",
    "        Args:\n",
    "            h (int): Original height.\n",
    "            w (int): Original width.\n",
    "            min_side_length (int): Minimum side length.\n",
    "            max_side_length (int): Maximum side length.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (new height, new width, resize ratio)\n",
    "        \"\"\"\n",
    "        if min(h, w) < min_side_length:\n",
    "            ratio = min_side_length / min(h, w)\n",
    "        elif max(h, w) > max_side_length:\n",
    "            ratio = max_side_length / max(h, w)\n",
    "        else:\n",
    "            ratio = 1.0\n",
    "        return int(h * ratio), int(w * ratio), ratio\n",
    "\n",
    "    @staticmethod\n",
    "    def resize(dataset_name: str, set_type: str, min_side_length: int, max_side_length: int, dotmap_together=True, is_gray=False):\n",
    "        \"\"\"\n",
    "        Batch processes images and dot maps in the dataset by resizing them.\n",
    "\n",
    "        Args:\n",
    "            dataset_name (str): Dataset name.\n",
    "            set_type (str): Data subset type ('train' or 'test').\n",
    "            min_side_length (int): Minimum side length for resizing.\n",
    "            max_side_length (int): Maximum side length for resizing.\n",
    "            dotmap_together (bool): Whether to resize the dot map together with the image.\n",
    "            is_gray (bool): Whether to load images as grayscale.\n",
    "        \"\"\"\n",
    "        prefix, img_suffix, dotmap_suffix = DirectoryPath.prefix_suffix(dataset_name)\n",
    "        \n",
    "        def resize_img(img_path, dotmap_path, dotmap_together, is_gray):\n",
    "            img = Image.open(img_path).convert('L') if is_gray else Image.open(img_path).convert('RGB')\n",
    "            dot_ann = np.load(dotmap_path) if dotmap_together else None\n",
    "\n",
    "            w, h = img.size\n",
    "            new_h, new_w, ratio = BatchImageDotmapProcessing.cal_new_size(h, w, min_side_length, max_side_length)\n",
    "            \n",
    "            if ratio != 1:\n",
    "                img = img.resize((new_w, new_h))\n",
    "                img.save(img_path, quality=95)\n",
    "                if dotmap_together and dot_ann.shape[0] > 0:\n",
    "                    dot_ann = dot_ann * ratio\n",
    "                    np.save(dotmap_path, dot_ann)\n",
    "                print(f\"Processed: {img_path}, {dotmap_path}\")\n",
    "\n",
    "        for img_path in glob.glob(os.path.join(prefix, set_type, img_suffix, '*.jpg')):\n",
    "            num = DirectoryPath.get_name_from_no(dataset_name, set_type, prefix, img_suffix, dotmap_suffix, os.path.basename(img_path).split('_')[-1].split('.')[0])\n",
    "            resize_img(img_path, num[1], dotmap_together, is_gray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "460153ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_1.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_10.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_100.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_101.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_102.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_103.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_104.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_105.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_106.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_107.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_108.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_109.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_11.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_110.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_111.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_112.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_113.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_114.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_115.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_116.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_117.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_118.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_119.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_12.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_120.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_121.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_122.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_123.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_124.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_125.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_126.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_127.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_128.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_129.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_13.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_130.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_131.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_132.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_133.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_134.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_135.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_136.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_137.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_138.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_139.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_14.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_140.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_141.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_142.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_143.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_144.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_145.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_146.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_147.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_148.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_149.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_15.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_150.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_151.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_152.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_153.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_154.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_155.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_156.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_157.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_158.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_159.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_16.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_160.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_161.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_162.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_163.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_164.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_165.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_166.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_167.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_168.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_169.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_17.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_170.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_171.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_172.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_173.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_174.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_175.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_176.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_177.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_178.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_179.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_18.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_180.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_181.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_182.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_183.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_184.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_185.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_186.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_187.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_188.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_189.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_19.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_190.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_191.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_192.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_193.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_194.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_195.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_196.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_197.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_198.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_199.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_2.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_20.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_200.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_201.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_202.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_203.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_204.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_205.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_206.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_207.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_208.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_209.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_21.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_210.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_211.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_212.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_213.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_214.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_215.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_216.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_217.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_218.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_219.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_22.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_220.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_221.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_222.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_223.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_224.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_225.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_226.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_227.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_228.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_229.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_23.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_230.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_231.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_232.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_233.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_234.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_235.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_236.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_237.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_238.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_239.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_24.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_240.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_241.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_242.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_243.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_244.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_245.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_246.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_247.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_248.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_249.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_25.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_250.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_251.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_252.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_253.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_254.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_255.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_256.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_257.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_258.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_259.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_26.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_260.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_261.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_262.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_263.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_264.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_265.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_266.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_267.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_268.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_269.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_27.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_270.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_271.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_272.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_273.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_274.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_275.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_276.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_277.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_278.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_279.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_28.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_280.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_281.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_282.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_283.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_284.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_285.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_286.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_287.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_288.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_289.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_29.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_290.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_291.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_292.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_293.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_294.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_295.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_296.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_297.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_298.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_299.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_3.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_30.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_300.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_31.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_32.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_33.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_34.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_35.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_36.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_37.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_38.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_39.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_4.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_40.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_41.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_42.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_43.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_44.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_45.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_46.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_47.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_48.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_49.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_5.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_50.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_51.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_52.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_53.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_54.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_55.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_56.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_57.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_58.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_59.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_6.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_60.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_61.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_62.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_63.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_64.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_65.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_66.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_67.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_68.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_69.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_7.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_70.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_71.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_72.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_73.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_74.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_75.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_76.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_77.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_78.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_79.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_8.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_80.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_81.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_82.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_83.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_84.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_85.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_86.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_87.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_88.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_89.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_9.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_90.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_91.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_92.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_93.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_94.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_95.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_96.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_97.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_98.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\train_data\\ground_truth_npy\\GT_IMG_99.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_1.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_10.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_100.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_101.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_102.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_103.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_104.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_105.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_106.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_107.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_108.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_109.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_11.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_110.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_111.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_112.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_113.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_114.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_115.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_116.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_117.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_118.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_119.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_12.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_120.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_121.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_122.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_123.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_124.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_125.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_126.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_127.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_128.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_129.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_13.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_130.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_131.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_132.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_133.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_134.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_135.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_136.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_137.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_138.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_139.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_14.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_140.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_141.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_142.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_143.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_144.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_145.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_146.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_147.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_148.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_149.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_15.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_150.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_151.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_152.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_153.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_154.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_155.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_156.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_157.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_158.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_159.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_16.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_160.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_161.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_162.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_163.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_164.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_165.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_166.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_167.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_168.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_169.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_17.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_170.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_171.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_172.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_173.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_174.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_175.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_176.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_177.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_178.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_179.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_18.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_180.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_181.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_182.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_19.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_2.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_20.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_21.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_22.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_23.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_24.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_25.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_26.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_27.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_28.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_29.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_3.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_30.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_31.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_32.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_33.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_34.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_35.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_36.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_37.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_38.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_39.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_4.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_40.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_41.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_42.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_43.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_44.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_45.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_46.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_47.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_48.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_49.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_5.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_50.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_51.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_52.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_53.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_54.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_55.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_56.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_57.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_58.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_59.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_6.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_60.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_61.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_62.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_63.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_64.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_65.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_66.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_67.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_68.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_69.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_7.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_70.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_71.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_72.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_73.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_74.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_75.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_76.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_77.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_78.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_79.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_8.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_80.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_81.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_82.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_83.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_84.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_85.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_86.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_87.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_88.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_89.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_9.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_90.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_91.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_92.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_93.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_94.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_95.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_96.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_97.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_98.npy\n",
      "Saved annotation to: C:\\Users\\hp\\Downloads\\ShanghaiTech_Crowd_Counting_Dataset\\part_A_final\\test_data\\ground_truth_npy\\GT_IMG_99.npy\n"
     ]
    }
   ],
   "source": [
    " DatasetPreparation.SHTCA() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4eca616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ChfLoss(tf.keras.layers.Layer):\n",
    "    def __init__(self, chf_step: int, chf_tik: float, sample_step: float, is_dense: bool):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chf_step (int): Number of steps the characteristic function spans in each direction.\n",
    "            chf_tik (float): Span of each step, determining the range of the characteristic function.\n",
    "            sample_step (float): Sampling interval for the image plane.\n",
    "            is_dense (bool): Whether the dataset is dense, affecting the choice of loss function.\n",
    "        \"\"\"\n",
    "        super(ChfLoss, self).__init__()\n",
    "        self.chf_step = chf_step\n",
    "        self.chf_tik = chf_tik\n",
    "        self.sample_step = sample_step\n",
    "        self.is_dense = is_dense\n",
    "\n",
    "        self.plane_shape = None\n",
    "        self.real_template = None\n",
    "        self.img_template = None\n",
    "\n",
    "    def make_template(self, dnn_output):\n",
    "        # Construct the spatial domain\n",
    "        x_axis = tf.linspace(self.sample_step / 2, dnn_output.shape[-1] * self.sample_step - self.sample_step / 2, dnn_output.shape[-1])\n",
    "        y_axis = tf.linspace(self.sample_step / 2, dnn_output.shape[-2] * self.sample_step - self.sample_step / 2, dnn_output.shape[-2])\n",
    "        \n",
    "        # Create sample coordinates\n",
    "        sample_coordinates = tf.stack([tf.repeat(x_axis, len(y_axis)), tf.tile(y_axis, [len(x_axis)])], axis=0)\n",
    "        sample_coordinates = tf.cast(sample_coordinates, dnn_output.dtype)\n",
    "        \n",
    "        # Construct characteristic function (frequency domain) plane\n",
    "        grid_x = tf.range(-self.chf_step, self.chf_step, dtype=dnn_output.dtype) * self.chf_tik\n",
    "        grid_y = tf.range(-self.chf_step, self.chf_step, dtype=dnn_output.dtype) * self.chf_tik\n",
    "        plane = tf.stack(tf.meshgrid(grid_x, grid_y), axis=-1)\n",
    "        \n",
    "        # Calculate the angle for characteristic function templates\n",
    "        angle = tf.einsum('ijk,lm->ijlm', plane, sample_coordinates)\n",
    "        self.real_template = tf.cos(angle)\n",
    "        self.img_template = tf.sin(angle)\n",
    "\n",
    "    def call(self, dnn_output, chf):\n",
    "        if self.plane_shape is None or self.plane_shape != dnn_output.shape[-2:]:\n",
    "            self.make_template(dnn_output)\n",
    "            self.plane_shape = dnn_output.shape[-2:]\n",
    "        \n",
    "        # Compute the characteristic function of the prediction\n",
    "        flatten_output = tf.reshape(dnn_output, [dnn_output.shape[0], -1])\n",
    "        chf_real = tf.reduce_sum(self.real_template * tf.expand_dims(flatten_output, axis=-1), axis=2)\n",
    "        chf_img = tf.reduce_sum(self.img_template * tf.expand_dims(flatten_output, axis=-1), axis=2)\n",
    "        derived_chf = tf.concat([tf.expand_dims(chf_real, -1), tf.expand_dims(chf_img, -1)], axis=-1)\n",
    "\n",
    "        # Choose loss based on density\n",
    "        if not self.is_dense:\n",
    "            loss = tf.reduce_sum(tf.norm(tf.reshape(derived_chf - chf, [chf.shape[0], -1]), axis=1) * self.chf_tik)\n",
    "        else:\n",
    "            loss = tf.reduce_sum(tf.norm(derived_chf - chf, axis=-1)) * self.chf_tik ** 2\n",
    "\n",
    "        return loss / tf.cast(chf.shape[0], dtype=chf.dtype)\n",
    "\n",
    "\n",
    "class ChfLikelihoodLoss(tf.keras.layers.Layer):\n",
    "    def __init__(self, chf_step: int, chf_tik: float, sample_step: float, likelihood):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chf_step (int): Number of steps the characteristic function spans in each direction.\n",
    "            chf_tik (float): Span of each step, determining the range of the characteristic function.\n",
    "            sample_step (float): Sampling interval for the image plane.\n",
    "            likelihood: Likelihood function to calculate loss.\n",
    "        \"\"\"\n",
    "        super(ChfLikelihoodLoss, self).__init__()\n",
    "        self.chf_step = chf_step\n",
    "        self.chf_tik = chf_tik\n",
    "        self.sample_step = sample_step\n",
    "        self.likelihood = likelihood\n",
    "        self.scale = 1\n",
    "\n",
    "        self.plane_shape = None\n",
    "        self.real_template = None\n",
    "        self.img_template = None\n",
    "\n",
    "    def make_template(self, dnn_output):\n",
    "        # Construct the spatial domain\n",
    "        x_axis = tf.linspace(self.sample_step / 2, dnn_output.shape[-1] * self.sample_step - self.sample_step / 2, dnn_output.shape[-1])\n",
    "        y_axis = tf.linspace(self.sample_step / 2, dnn_output.shape[-2] * self.sample_step - self.sample_step / 2, dnn_output.shape[-2])\n",
    "        \n",
    "        # Create sample coordinates\n",
    "        sample_coordinates = tf.stack([tf.repeat(x_axis, len(y_axis)), tf.tile(y_axis, [len(x_axis)])], axis=0)\n",
    "        sample_coordinates = tf.cast(sample_coordinates, dnn_output.dtype)\n",
    "        \n",
    "        # Construct characteristic function (frequency domain) plane\n",
    "        grid_x = tf.range(-self.chf_step, self.chf_step, dtype=dnn_output.dtype) * self.chf_tik\n",
    "        grid_y = tf.range(-self.chf_step, self.chf_step, dtype=dnn_output.dtype) * self.chf_tik\n",
    "        plane = tf.stack(tf.meshgrid(grid_x, grid_y), axis=-1)\n",
    "        \n",
    "        # Calculate the angle for characteristic function templates\n",
    "        angle = tf.einsum('ijk,lm->ijlm', plane, sample_coordinates)\n",
    "        self.real_template = tf.cos(angle)\n",
    "        self.img_template = tf.sin(angle)\n",
    "\n",
    "    def call(self, dnn_output, chf):\n",
    "        if self.plane_shape is None or self.plane_shape != dnn_output.shape:\n",
    "            self.make_template(dnn_output)\n",
    "            self.plane_shape = dnn_output.shape\n",
    "\n",
    "        # Compute the characteristic function of the prediction\n",
    "        flatten_output = tf.reshape(dnn_output, [dnn_output.shape[0], -1])\n",
    "        chf_real = tf.reduce_sum(self.real_template * tf.expand_dims(flatten_output, axis=-1), axis=2)\n",
    "        chf_img = tf.reduce_sum(self.img_template * tf.expand_dims(flatten_output, axis=-1), axis=2)\n",
    "        derived_chf = tf.concat([tf.expand_dims(chf_real, -1), tf.expand_dims(chf_img, -1)], axis=-1)\n",
    "\n",
    "        # Use the likelihood function to calculate loss\n",
    "        loss = self.likelihood(derived_chf, chf, self.scale)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a12baeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ChfLoss(tf.keras.layers.Layer):\n",
    "    def __init__(self, chf_step: int, chf_tik: float, sample_step: float, is_dense: bool):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chf_step (int): Number of steps the characteristic function spans in each direction.\n",
    "            chf_tik (float): Span of each step, determining the range of the characteristic function.\n",
    "            sample_step (float): Sampling interval for the image plane.\n",
    "            is_dense (bool): Whether the dataset is dense, affecting the choice of loss function.\n",
    "        \"\"\"\n",
    "        super(ChfLoss, self).__init__()\n",
    "        self.chf_step = chf_step\n",
    "        self.chf_tik = chf_tik\n",
    "        self.sample_step = sample_step\n",
    "        self.is_dense = is_dense\n",
    "\n",
    "        self.plane_shape = None\n",
    "        self.real_template = None\n",
    "        self.img_template = None\n",
    "\n",
    "    def make_template(self, dnn_output):\n",
    "        # Construct the spatial domain\n",
    "        x_axis = tf.linspace(self.sample_step / 2, dnn_output.shape[-1] * self.sample_step - self.sample_step / 2, dnn_output.shape[-1])\n",
    "        y_axis = tf.linspace(self.sample_step / 2, dnn_output.shape[-2] * self.sample_step - self.sample_step / 2, dnn_output.shape[-2])\n",
    "        \n",
    "        # Create sample coordinates\n",
    "        sample_coordinates = tf.stack([tf.repeat(x_axis, len(y_axis)), tf.tile(y_axis, [len(x_axis)])], axis=0)\n",
    "        sample_coordinates = tf.cast(sample_coordinates, dnn_output.dtype)\n",
    "        \n",
    "        # Construct characteristic function (frequency domain) plane\n",
    "        grid_x = tf.range(-self.chf_step, self.chf_step, dtype=dnn_output.dtype) * self.chf_tik\n",
    "        grid_y = tf.range(-self.chf_step, self.chf_step, dtype=dnn_output.dtype) * self.chf_tik\n",
    "        plane = tf.stack(tf.meshgrid(grid_x, grid_y), axis=-1)\n",
    "        \n",
    "        # Calculate the angle for characteristic function templates\n",
    "        angle = tf.einsum('ijk,lm->ijlm', plane, sample_coordinates)\n",
    "        self.real_template = tf.cos(angle)\n",
    "        self.img_template = tf.sin(angle)\n",
    "\n",
    "    def call(self, dnn_output, chf):\n",
    "        if self.plane_shape is None or self.plane_shape != dnn_output.shape[-2:]:\n",
    "            self.make_template(dnn_output)\n",
    "            self.plane_shape = dnn_output.shape[-2:]\n",
    "        \n",
    "        # Compute the characteristic function of the prediction\n",
    "        flatten_output = tf.reshape(dnn_output, [dnn_output.shape[0], -1])\n",
    "        chf_real = tf.reduce_sum(self.real_template * tf.expand_dims(flatten_output, axis=-1), axis=2)\n",
    "        chf_img = tf.reduce_sum(self.img_template * tf.expand_dims(flatten_output, axis=-1), axis=2)\n",
    "        derived_chf = tf.concat([tf.expand_dims(chf_real, -1), tf.expand_dims(chf_img, -1)], axis=-1)\n",
    "\n",
    "        # Choose loss based on density\n",
    "        if not self.is_dense:\n",
    "            loss = tf.reduce_sum(tf.norm(tf.reshape(derived_chf - chf, [chf.shape[0], -1]), axis=1) * self.chf_tik)\n",
    "        else:\n",
    "            loss = tf.reduce_sum(tf.norm(derived_chf - chf, axis=-1)) * self.chf_tik ** 2\n",
    "\n",
    "        return loss / tf.cast(chf.shape[0], dtype=chf.dtype)\n",
    "\n",
    "\n",
    "class ChfLikelihoodLoss(tf.keras.layers.Layer):\n",
    "    def __init__(self, chf_step: int, chf_tik: float, sample_step: float, likelihood):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chf_step (int): Number of steps the characteristic function spans in each direction.\n",
    "            chf_tik (float): Span of each step, determining the range of the characteristic function.\n",
    "            sample_step (float): Sampling interval for the image plane.\n",
    "            likelihood: Likelihood function to calculate loss.\n",
    "        \"\"\"\n",
    "        super(ChfLikelihoodLoss, self).__init__()\n",
    "        self.chf_step = chf_step\n",
    "        self.chf_tik = chf_tik\n",
    "        self.sample_step = sample_step\n",
    "        self.likelihood = likelihood\n",
    "        self.scale = 1\n",
    "\n",
    "        self.plane_shape = None\n",
    "        self.real_template = None\n",
    "        self.img_template = None\n",
    "\n",
    "    def make_template(self, dnn_output):\n",
    "        # Construct the spatial domain\n",
    "        x_axis = tf.linspace(self.sample_step / 2, dnn_output.shape[-1] * self.sample_step - self.sample_step / 2, dnn_output.shape[-1])\n",
    "        y_axis = tf.linspace(self.sample_step / 2, dnn_output.shape[-2] * self.sample_step - self.sample_step / 2, dnn_output.shape[-2])\n",
    "        \n",
    "        # Create sample coordinates\n",
    "        sample_coordinates = tf.stack([tf.repeat(x_axis, len(y_axis)), tf.tile(y_axis, [len(x_axis)])], axis=0)\n",
    "        sample_coordinates = tf.cast(sample_coordinates, dnn_output.dtype)\n",
    "        \n",
    "        # Construct characteristic function (frequency domain) plane\n",
    "        grid_x = tf.range(-self.chf_step, self.chf_step, dtype=dnn_output.dtype) * self.chf_tik\n",
    "        grid_y = tf.range(-self.chf_step, self.chf_step, dtype=dnn_output.dtype) * self.chf_tik\n",
    "        plane = tf.stack(tf.meshgrid(grid_x, grid_y), axis=-1)\n",
    "        \n",
    "        # Calculate the angle for characteristic function templates\n",
    "        angle = tf.einsum('ijk,lm->ijlm', plane, sample_coordinates)\n",
    "        self.real_template = tf.cos(angle)\n",
    "        self.img_template = tf.sin(angle)\n",
    "\n",
    "    def call(self, dnn_output, chf):\n",
    "        if self.plane_shape is None or self.plane_shape != dnn_output.shape:\n",
    "            self.make_template(dnn_output)\n",
    "            self.plane_shape = dnn_output.shape\n",
    "\n",
    "        # Compute the characteristic function of the prediction\n",
    "        flatten_output = tf.reshape(dnn_output, [dnn_output.shape[0], -1])\n",
    "        chf_real = tf.reduce_sum(self.real_template * tf.expand_dims(flatten_output, axis=-1), axis=2)\n",
    "        chf_img = tf.reduce_sum(self.img_template * tf.expand_dims(flatten_output, axis=-1), axis=2)\n",
    "        derived_chf = tf.concat([tf.expand_dims(chf_real, -1), tf.expand_dims(chf_img, -1)], axis=-1)\n",
    "\n",
    "        # Use the likelihood function to calculate loss\n",
    "        loss = self.likelihood(derived_chf, chf, self.scale)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "016767a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, dnn_output, chf):\n",
    "    if self.plane_shape is None or self.plane_shape != dnn_output.shape[-2:]:\n",
    "        self.make_template(dnn_output)\n",
    "        self.plane_shape = dnn_output.shape[-2:]\n",
    "    \n",
    "    # Compute the characteristic function of the prediction\n",
    "    flatten_output = tf.reshape(dnn_output, [dnn_output.shape[0], -1, 1])  # Reshape to [batch, flattened, 1] for broadcasting\n",
    "    \n",
    "    # Adjust self.real_template and self.img_template to include batch dimension\n",
    "    real_template_expanded = tf.expand_dims(self.real_template, axis=0)  # Shape: [1, 2*chf_step, 2*chf_step, num_pixels]\n",
    "    img_template_expanded = tf.expand_dims(self.img_template, axis=0)    # Shape: [1, 2*chf_step, 2*chf_step, num_pixels]\n",
    "    \n",
    "    # Calculate real and imaginary parts of the derived characteristic function\n",
    "    chf_real = tf.reduce_sum(real_template_expanded * flatten_output, axis=3, keepdims=True)\n",
    "    chf_img = tf.reduce_sum(img_template_expanded * flatten_output, axis=3, keepdims=True)\n",
    "    derived_chf = tf.concat([chf_real, chf_img], axis=3)\n",
    "    \n",
    "    # Choose loss based on density\n",
    "    if not self.is_dense:\n",
    "        loss = tf.reduce_sum(tf.norm(tf.reshape(derived_chf - chf, [chf.shape[0], -1]), axis=1) * self.chf_tik)\n",
    "    else:\n",
    "        loss = tf.reduce_sum(tf.norm(derived_chf - chf, axis=2)) * self.chf_tik ** 2\n",
    "\n",
    "    return loss / tf.cast(chf.shape[0], dtype=chf.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dabdf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a99aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, dnn_output, chf):\n",
    "    if self.plane_shape is None or self.plane_shape != dnn_output.shape[-2:]:\n",
    "        self.make_template(dnn_output)\n",
    "        self.plane_shape = dnn_output.shape[-2:]\n",
    "    \n",
    "    # Flatten dnn_output and ensure compatible shape with real_template and img_template\n",
    "    flatten_output = tf.reshape(dnn_output, [dnn_output.shape[0], -1, 1])  # Shape: [batch, num_pixels, 1]\n",
    "    \n",
    "    # Expand dimensions of templates to match with flattened DNN output\n",
    "    real_template_expanded = tf.expand_dims(self.real_template, axis=0)  # Shape: [1, 2*chf_step, 2*chf_step, num_pixels]\n",
    "    img_template_expanded = tf.expand_dims(self.img_template, axis=0)    # Shape: [1, 2*chf_step, 2*chf_step, num_pixels]\n",
    "\n",
    "    # Make sure templates are compatible with the batch size and flatten_output\n",
    "    real_template_expanded = tf.broadcast_to(real_template_expanded, [dnn_output.shape[0], *self.real_template.shape])\n",
    "    img_template_expanded = tf.broadcast_to(img_template_expanded, [dnn_output.shape[0], *self.img_template.shape])\n",
    "\n",
    "    # Multiply and sum across appropriate axis\n",
    "    chf_real = tf.reduce_sum(real_template_expanded * flatten_output, axis=2, keepdims=True)\n",
    "    chf_img = tf.reduce_sum(img_template_expanded * flatten_output, axis=2, keepdims=True)\n",
    "    derived_chf = tf.concat([chf_real, chf_img], axis=3)\n",
    "\n",
    "    # Choose loss function based on density\n",
    "    if not self.is_dense:\n",
    "        loss = tf.reduce_sum(tf.norm(tf.reshape(derived_chf - chf, [chf.shape[0], -1]), axis=1) * self.chf_tik)\n",
    "    else:\n",
    "        loss = tf.reduce_sum(tf.norm(derived_chf - chf, axis=2)) * (self.chf_tik ** 2)\n",
    "\n",
    "    return loss / tf.cast(chf.shape[0], dtype=chf.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38d982f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-hub) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-hub) (4.25.5)\n",
      "Collecting tf-keras>=2.14.1\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 12.2 MB/s eta 0:00:00\n",
      "Collecting tensorflow<2.19,>=2.18\n",
      "  Downloading tensorflow-2.18.0-cp39-cp39-win_amd64.whl (7.5 kB)\n",
      "Collecting tensorflow-intel==2.18.0\n",
      "  Downloading tensorflow_intel-2.18.0-cp39-cp39-win_amd64.whl (390.0 MB)\n",
      "     -------------------------------------- 390.0/390.0 MB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.4.0)\n",
      "Collecting tensorboard<2.19,>=2.18\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 5.5/5.5 MB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (21.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.12.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.66.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (63.4.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.14.1)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.6.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (4.12.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.28.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.31.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.5.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.37.1)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.13.0)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (13.9.2)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.0.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.3.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.0.9)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.1.2)\n",
      "Installing collected packages: tensorboard, tensorflow-intel, tensorflow, tf-keras, tensorflow-hub\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.1\n",
      "    Uninstalling tensorboard-2.17.1:\n",
      "      Successfully uninstalled tensorboard-2.17.1\n",
      "  Attempting uninstall: tensorflow-intel\n",
      "    Found existing installation: tensorflow-intel 2.17.0\n",
      "    Uninstalling tensorflow-intel-2.17.0:\n",
      "      Successfully uninstalled tensorflow-intel-2.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\hp\\\\anaconda3\\\\Lib\\\\site-packages\\\\~ensorflow\\\\compiler\\\\mlir\\\\lite\\\\python\\\\_pywrap_converter_api.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db58ea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-trained weights provided. Using default initialization.\n",
      "Output shape: (1, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Define the configuration for VGG layers\n",
    "cfg = {\n",
    "    'Baysian_Ma': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]\n",
    "}\n",
    "\n",
    "# Helper function to create VGG layers\n",
    "def make_layers(cfg, use_batch_norm=True):\n",
    "    \"\"\"Creates layers based on configuration, similar to PyTorch make_layers\"\"\"\n",
    "    layers_list = []\n",
    "    in_channels = 3  # Initial input channel count for RGB images\n",
    "    \n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers_list.append(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        else:\n",
    "            conv_layer = layers.Conv2D(v, kernel_size=3, padding=\"same\")\n",
    "            if use_batch_norm:\n",
    "                layers_list.append(conv_layer)\n",
    "                layers_list.append(layers.BatchNormalization())\n",
    "                layers_list.append(layers.ReLU())\n",
    "            else:\n",
    "                layers_list.append(conv_layer)\n",
    "                layers_list.append(layers.ReLU())\n",
    "    return tf.keras.Sequential(layers_list)\n",
    "\n",
    "# Define the VGG-based model for Bayesian Ma\n",
    "class VGG_Baysian_Ma(Model):\n",
    "    def __init__(self, feature_layers):\n",
    "        super(VGG_Baysian_Ma, self).__init__()\n",
    "        self.features = feature_layers\n",
    "        self.reg_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(256, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "            layers.Conv2D(128, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "            layers.Conv2D(1, kernel_size=1, padding=\"same\", activation=\"relu\")\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.features(x)\n",
    "        x = tf.image.resize(x, [tf.shape(x)[1] * 2, tf.shape(x)[2] * 2], method=\"bilinear\")\n",
    "        x = self.reg_layer(x)\n",
    "        return tf.abs(x)\n",
    "\n",
    "def vgg19(use_batch_norm=True, layers='Baysian_Ma', weights=None):\n",
    "    \"\"\"\n",
    "    Creates a VGG 19-layer model with optional batch normalization.\n",
    "    The model is compatible with the Bayesian Ma loss function for crowd counting.\n",
    "    \"\"\"\n",
    "    # Initialize the feature extractor layers\n",
    "    feature_layers = make_layers(cfg[layers], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    # Initialize the VGG model\n",
    "    model = VGG_Baysian_Ma(feature_layers)\n",
    "    \n",
    "    # Load weights if provided\n",
    "    if weights:\n",
    "        model.load_weights(weights)\n",
    "    else:\n",
    "        print(\"No pre-trained weights provided. Using default initialization.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "model = vgg19()\n",
    "input_tensor = tf.random.normal([1, 256, 256, 3])  # Batch size 1, 256x256 RGB image\n",
    "output = model(input_tensor)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2947b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6b400b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShanghaiTech Part A (SHTCA) dataset is prepared.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "\n",
    "# Define Dataset Preparation class and methods\n",
    "class Dataset_preparation:\n",
    "    @staticmethod\n",
    "    def SHTCA():\n",
    "        \"\"\"Prepares the ShanghaiTech Part A dataset by converting `.mat` files to `.npy` format.\"\"\"\n",
    "        Trans_gt_to_ndarray.trans_ann_to_npy_SHTC(\n",
    "            'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\ground_truth',\n",
    "            'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\1'\n",
    "        )\n",
    "        Trans_gt_to_ndarray.trans_ann_to_npy_SHTC(\n",
    "            'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\test_data\\\\ground_truth',\n",
    "            'C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\test_data\\\\11'\n",
    "        )\n",
    "        print('ShanghaiTech Part A (SHTCA) dataset is prepared.')\n",
    "\n",
    "class Trans_gt_to_ndarray:\n",
    "    @staticmethod\n",
    "    def trans_ann_to_npy_SHTC(target_path: str, save_path: str):\n",
    "        \"\"\"Converts ShanghaiTech Part A `.mat` annotation files to `.npy` format.\"\"\"\n",
    "        assert target_path != save_path, \"Target and save paths must be different.\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "            \n",
    "        for file in glob.glob(target_path + '/*.mat'):\n",
    "            dot = loadmat(file)\n",
    "            x = dot[list(dot.keys())[-1]][0, 0]['location'][0, 0].astype(np.float32)\n",
    "            np.save(os.path.join(save_path, os.path.basename(file).split('.')[0] + '.npy'), x)\n",
    "\n",
    "# Run the dataset preparation for SHTCA\n",
    "if __name__ == '__main__':\n",
    "    Dataset_preparation.SHTCA()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "faa03941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration for VGG layers\n",
    "def make_vgg_layers(config, batch_norm=True):\n",
    "    vgg_layers = []\n",
    "    for v in config:\n",
    "        if v == 'M':  # Apply MaxPooling only when 'M' is encountered\n",
    "            vgg_layers.append(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        else:  # Only pass integers for Conv2D filters\n",
    "            conv2d = layers.Conv2D(int(v), kernel_size=(3, 3), padding='same')\n",
    "            if batch_norm:\n",
    "                vgg_layers.extend([conv2d, layers.BatchNormalization(), layers.ReLU()])\n",
    "            else:\n",
    "                vgg_layers.extend([conv2d, layers.ReLU()])\n",
    "    return vgg_layers\n",
    "\n",
    "# VGG model definition\n",
    "config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]\n",
    "\n",
    "def vgg19():\n",
    "    inputs = layers.Input(shape=(None, None, 3))\n",
    "    x = inputs\n",
    "    for layer in make_vgg_layers(config):\n",
    "        x = layer(x)\n",
    "    x = layers.Conv2D(256, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(128, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(1, kernel_size=(1, 1))(x)\n",
    "    model = Model(inputs=inputs, outputs=tf.abs(x))  # Apply absolute value to match PyTorch's torch.abs\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750ab20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7eade0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f0fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CHFLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, chf_step: int, chf_tik: float, sample_step: float, is_dense: bool, name=\"chf_loss\"):\n",
    "        super(CHFLoss, self).__init__(name=name)\n",
    "        self.chf_step = chf_step\n",
    "        self.chf_tik = chf_tik\n",
    "        self.sample_step = sample_step\n",
    "        self.is_dense = is_dense\n",
    "        self.real_template = None\n",
    "        self.img_template = None\n",
    "\n",
    "    def build_templates(self, dnn_output_shape):\n",
    "        x_axis = tf.linspace(\n",
    "            self.sample_step / 2, \n",
    "            dnn_output_shape[2] * self.sample_step - self.sample_step / 2, \n",
    "            dnn_output_shape[2]\n",
    "        )\n",
    "        y_axis = tf.linspace(\n",
    "            self.sample_step / 2, \n",
    "            dnn_output_shape[1] * self.sample_step - self.sample_step / 2, \n",
    "            dnn_output_shape[1]\n",
    "        )\n",
    "        # Flatten sample coordinates for use in einsum\n",
    "        sample_coordinates = tf.reshape(\n",
    "            tf.stack([tf.repeat(x_axis, len(y_axis)), tf.tile(y_axis, [len(x_axis)])], axis=0),\n",
    "            [2, -1]\n",
    "        )\n",
    "\n",
    "        # Create plane for characteristic function\n",
    "        plane_x = tf.range(-self.chf_step, self.chf_step, dtype=tf.float32) * self.chf_tik\n",
    "        plane_y = tf.range(-self.chf_step, self.chf_step, dtype=tf.float32) * self.chf_tik\n",
    "        plane = tf.reshape(tf.stack(tf.meshgrid(plane_x, plane_y), axis=-1), [-1, 2])\n",
    "\n",
    "        # Compute angle matrix\n",
    "        angle = tf.einsum('ij,jk->ik', plane, sample_coordinates)\n",
    "\n",
    "        # Build real and imaginary templates\n",
    "        self.real_template = tf.cos(angle)\n",
    "        self.img_template = tf.sin(angle)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        if self.real_template is None or self.img_template is None:\n",
    "            self.build_templates(y_pred.shape)\n",
    "\n",
    "        flatten_pred = tf.reshape(y_pred, [y_pred.shape[0], -1])\n",
    "        chf_real = tf.reduce_sum(self.real_template * tf.expand_dims(flatten_pred, axis=-1), axis=1)\n",
    "        chf_img = tf.reduce_sum(self.img_template * tf.expand_dims(flatten_pred, axis=-1), axis=1)\n",
    "        derived_chf = tf.stack([chf_real, chf_img], axis=-1)\n",
    "\n",
    "        if not self.is_dense:\n",
    "            loss = tf.reduce_sum(tf.norm(derived_chf - y_true, axis=-1)) * self.chf_tik\n",
    "        else:\n",
    "            loss = tf.reduce_sum(tf.square(derived_chf - y_true)) * (self.chf_tik ** 2)\n",
    "        \n",
    "        return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff4c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "574526b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CHFLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, chf_step: int, chf_tik: float, sample_step: float, is_dense: bool, name=\"chf_loss\"):\n",
    "        super(CHFLoss, self).__init__(name=name)\n",
    "        self.chf_step = chf_step\n",
    "        self.chf_tik = chf_tik\n",
    "        self.sample_step = sample_step\n",
    "        self.is_dense = is_dense\n",
    "        self.real_template = None\n",
    "        self.img_template = None\n",
    "\n",
    "    def build_templates(self, dnn_output_shape):\n",
    "        x_axis = tf.linspace(\n",
    "            self.sample_step / 2, \n",
    "            dnn_output_shape[2] * self.sample_step - self.sample_step / 2, \n",
    "            dnn_output_shape[2]\n",
    "        )\n",
    "        y_axis = tf.linspace(\n",
    "            self.sample_step / 2, \n",
    "            dnn_output_shape[1] * self.sample_step - self.sample_step / 2, \n",
    "            dnn_output_shape[1]\n",
    "        )\n",
    "        # Flatten sample coordinates for use in einsum\n",
    "        sample_coordinates = tf.reshape(\n",
    "            tf.stack([tf.repeat(x_axis, len(y_axis)), tf.tile(y_axis, [len(x_axis)])], axis=0),\n",
    "            [2, -1]\n",
    "        )\n",
    "\n",
    "        # Create plane for characteristic function\n",
    "        plane_x = tf.range(-self.chf_step, self.chf_step, dtype=tf.float32) * self.chf_tik\n",
    "        plane_y = tf.range(-self.chf_step, self.chf_step, dtype=tf.float32) * self.chf_tik\n",
    "        plane = tf.reshape(tf.stack(tf.meshgrid(plane_x, plane_y), axis=-1), [-1, 2])\n",
    "\n",
    "        # Compute angle matrix\n",
    "        angle = tf.einsum('ij,jk->ik', plane, sample_coordinates)\n",
    "\n",
    "        # Build real and imaginary templates with an extra batch dimension\n",
    "        self.real_template = tf.cos(angle)\n",
    "        self.img_template = tf.sin(angle)\n",
    "        # Reshape templates to have a batch dimension for later broadcasting\n",
    "        self.real_template = tf.reshape(self.real_template, [1, -1, angle.shape[1]])\n",
    "        self.img_template = tf.reshape(self.img_template, [1, -1, angle.shape[1]])\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        if self.real_template is None or self.img_template is None:\n",
    "            self.build_templates(y_pred.shape)\n",
    "\n",
    "        # Reshape prediction to match template shapes for multiplication\n",
    "        flatten_pred = tf.reshape(y_pred, [y_pred.shape[0], -1, 1])\n",
    "\n",
    "        chf_real = tf.reduce_sum(self.real_template * flatten_pred, axis=1)\n",
    "        chf_img = tf.reduce_sum(self.img_template * flatten_pred, axis=1)\n",
    "        derived_chf = tf.stack([chf_real, chf_img], axis=-1)\n",
    "\n",
    "        # Calculate loss\n",
    "        if not self.is_dense:\n",
    "            loss = tf.reduce_sum(tf.norm(derived_chf - y_true, axis=-1)) * self.chf_tik\n",
    "        else:\n",
    "            loss = tf.reduce_sum(tf.square(derived_chf - y_true)) * (self.chf_tik ** 2)\n",
    "        \n",
    "        return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a146c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff702ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class CHFLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, chf_step=30, chf_tik=0.01, sample_step=0.1, is_dense=True):\n",
    "        super(CHFLoss, self).__init__()\n",
    "        self.chf_step = chf_step\n",
    "        self.chf_tik = chf_tik\n",
    "        self.sample_step = sample_step\n",
    "        self.is_dense = is_dense\n",
    "        self.real_template = None\n",
    "        self.img_template = None\n",
    "\n",
    "    def build_templates(self, dnn_output_shape):\n",
    "        # Generate sample coordinates as a 1D vector compatible with einsum\n",
    "        sample_coordinates = tf.range(-self.chf_step, self.chf_step, delta=self.sample_step, dtype=tf.float32)\n",
    "        \n",
    "        # Generate a grid for plane coordinates\n",
    "        plane_x = tf.range(-self.chf_step, self.chf_step, dtype=tf.float32) * self.chf_tik\n",
    "        plane_y = tf.range(-self.chf_step, self.chf_step, dtype=tf.float32) * self.chf_tik\n",
    "        plane = tf.stack(tf.meshgrid(plane_x, plane_y), axis=-1)  # Shape: [M, M, 2]\n",
    "        plane = tf.reshape(plane, [-1, 2])  # Flatten to [M^2, 2] for einsum compatibility\n",
    "\n",
    "        # Calculate the angle matrix using matrix multiplication\n",
    "        angle = tf.einsum('ik,k->i', plane, sample_coordinates)  # Shape [M^2, N]\n",
    "        angle = tf.reshape(angle, [1, -1])  # Add batch dimension for broadcasting\n",
    "\n",
    "        # Create the real and imaginary CHF templates\n",
    "        self.real_template = tf.cos(angle)\n",
    "        self.img_template = tf.sin(angle)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        if self.real_template is None or self.img_template is None:\n",
    "            self.build_templates(y_pred.shape)\n",
    "\n",
    "        # Flatten y_pred and y_true to match template shapes\n",
    "        batch_size = tf.shape(y_pred)[0]\n",
    "        flatten_pred = tf.reshape(y_pred, [batch_size, -1])\n",
    "        flatten_true = tf.reshape(y_true, [batch_size, -1])\n",
    "\n",
    "        # Broadcast templates to match batch size\n",
    "        real_template = tf.tile(self.real_template, [batch_size, 1])\n",
    "        img_template = tf.tile(self.img_template, [batch_size, 1])\n",
    "\n",
    "        # Compute CHF real and imaginary components\n",
    "        chf_real = tf.reduce_sum(real_template * flatten_pred, axis=1)\n",
    "        chf_img = tf.reduce_sum(img_template * flatten_pred, axis=1)\n",
    "        derived_chf = tf.stack([chf_real, chf_img], axis=-1)\n",
    "\n",
    "        # Compute CHF for true values\n",
    "        true_chf_real = tf.reduce_sum(real_template * flatten_true, axis=1)\n",
    "        true_chf_img = tf.reduce_sum(img_template * flatten_true, axis=1)\n",
    "        true_chf = tf.stack([true_chf_real, true_chf_img], axis=-1)\n",
    "\n",
    "        # Calculate CHF loss\n",
    "        loss = tf.reduce_mean(tf.reduce_sum(tf.square(derived_chf - true_chf), axis=-1))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce64b947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "931b6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2abbdfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths for the dataset\n",
    "TRAIN_IMAGES_DIR = \"C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\images\"\n",
    "TRAIN_ANNOTATIONS_DIR = \"C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\ground_truth_npy\"\n",
    "TEST_IMAGES_DIR = \"C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\test_data\\\\images\"\n",
    "TEST_ANNOTATIONS_DIR = \"C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\test_data\\\\ground_truth_npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "762ed246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return image\n",
    "\n",
    "def load_annotation(annotation_path):\n",
    "    annotation = np.load(annotation_path.numpy().decode(\"utf-8\"))\n",
    "    return annotation.astype(np.float32)\n",
    "\n",
    "def process_data(img_path, ann_path, img_size=(256, 256)):\n",
    "    img = load_image(img_path)\n",
    "    ann = tf.py_function(load_annotation, [ann_path], tf.float32)\n",
    "    ann.set_shape([None, None])  # Set shape explicitly\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    ann = tf.image.resize(tf.expand_dims(ann, axis=-1), img_size)\n",
    "    return img, ann\n",
    "\n",
    "def prepare_datasets(batch_size=4, img_size=(256, 256)):\n",
    "    train_image_paths = glob.glob(os.path.join(TRAIN_IMAGES_DIR, \"*.jpg\"))\n",
    "    train_annotation_paths = [os.path.join(TRAIN_ANNOTATIONS_DIR, \"GT_\" + os.path.basename(img).replace(\".jpg\", \".npy\")) for img in train_image_paths]\n",
    "    test_image_paths = glob.glob(os.path.join(TEST_IMAGES_DIR, \"*.jpg\"))\n",
    "    test_annotation_paths = [os.path.join(TEST_ANNOTATIONS_DIR, \"GT_\" + os.path.basename(img).replace(\".jpg\", \".npy\")) for img in test_image_paths]\n",
    "    \n",
    "    train_data = tf.data.Dataset.from_tensor_slices((train_image_paths, train_annotation_paths))\n",
    "    train_data = train_data.map(lambda img, ann: process_data(img, ann, img_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_data = train_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    test_data = tf.data.Dataset.from_tensor_slices((test_image_paths, test_annotation_paths))\n",
    "    test_data = test_data.map(lambda img, ann: process_data(img, ann, img_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_data = test_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f6bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg19(input_shape=(256, 256, 3)):\n",
    "    config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for v in config:\n",
    "        if v == 'M':\n",
    "            x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "        else:\n",
    "            x = layers.Conv2D(v, kernel_size=(3, 3), padding='same')(x)\n",
    "            x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(256, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(128, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(1, kernel_size=(1, 1))(x)\n",
    "    x = layers.Lambda(lambda x: tf.abs(x))(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b6063e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHFLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, chf_step=30, chf_tik=0.01, sample_interval=0.1, is_dense=True):\n",
    "        super(CHFLoss, self).__init__()\n",
    "        self.chf_step = chf_step\n",
    "        self.chf_tik = chf_tik\n",
    "        self.sample_interval = sample_interval\n",
    "        self.is_dense = is_dense\n",
    "        self.real_template = None\n",
    "        self.img_template = None\n",
    "\n",
    "    def build_templates(self, dnn_output_shape):\n",
    "        sample_coordinates = tf.range(-self.chf_step, self.chf_step, delta=self.sample_interval, dtype=tf.float32)\n",
    "        sample_coordinates = tf.reshape(sample_coordinates, [-1, 1])\n",
    "        plane_x = tf.range(-self.chf_step, self.chf_step, dtype=tf.float32) * self.chf_tik\n",
    "        plane_y = tf.range(-self.chf_step, self.chf_step, dtype=tf.float32) * self.chf_tik\n",
    "        plane = tf.stack(tf.meshgrid(plane_x, plane_y), axis=-1)\n",
    "        plane = tf.reshape(plane, [-1, 2])\n",
    "        angle = tf.einsum('ij,jk->ik', plane, sample_coordinates)\n",
    "        self.real_template = tf.expand_dims(tf.cos(angle), axis=0)\n",
    "        self.img_template = tf.expand_dims(tf.sin(angle), axis=0)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        if self.real_template is None or self.img_template is None:\n",
    "            self.build_templates(y_pred.shape)\n",
    "        batch_size = tf.shape(y_pred)[0]\n",
    "        flatten_pred = tf.reshape(y_pred, [batch_size, -1])\n",
    "        flatten_real_template = tf.tile(self.real_template, [batch_size, 1, 1])\n",
    "        flatten_img_template = tf.tile(self.img_template, [batch_size, 1, 1])\n",
    "        chf_real = tf.reduce_sum(flatten_real_template * tf.expand_dims(flatten_pred, axis=-1), axis=1)\n",
    "        chf_img = tf.reduce_sum(flatten_img_template * tf.expand_dims(flatten_pred, axis=-1), axis=1)\n",
    "        derived_chf = tf.stack([chf_real, chf_img], axis=-1)\n",
    "        flatten_true = tf.reshape(y_true, [batch_size, -1])\n",
    "        true_chf_real = tf.reduce_sum(flatten_real_template * tf.expand_dims(flatten_true, axis=-1), axis=1)\n",
    "        true_chf_img = tf.reduce_sum(flatten_img_template * tf.expand_dims(flatten_true, axis=-1), axis=1)\n",
    "        true_chf = tf.stack([true_chf_real, true_chf_img], axis=-1)\n",
    "        loss = tf.reduce_mean(tf.reduce_sum(tf.square(derived_chf - true_chf), axis=-1))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43c47cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, test_data, chf_loss, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(x_batch_train, training=True)\n",
    "                predictions_resized = tf.image.resize(predictions, [256, 256])\n",
    "                loss_value = chf_loss(y_batch_train, predictions_resized)\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Step {step} - Loss: {loss_value.numpy():.4f}\")\n",
    "        mse_metric = tf.keras.metrics.MeanSquaredError()\n",
    "        mae_metric = tf.keras.metrics.MeanAbsoluteError()\n",
    "        for x_batch_test, y_batch_test in test_data:\n",
    "            test_predictions = model(x_batch_test, training=False)\n",
    "            test_predictions_resized = tf.image.resize(test_predictions, [256, 256])\n",
    "            mse_metric.update_state(y_batch_test, test_predictions_resized)\n",
    "            mae_metric.update_state(y_batch_test, test_predictions_resized)\n",
    "        print(f\"Epoch {epoch + 1} - MSE: {mse_metric.result().numpy()}, MAE: {mae_metric.result().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015536fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1e110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108adaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e288c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a8895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e79d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f71bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36785803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Step 0 - Loss: 93.7389\n",
      "Step 10 - Loss: 115.3169\n",
      "Step 20 - Loss: 94.5600\n",
      "Step 30 - Loss: 94.0384\n",
      "Step 40 - Loss: 107.8531\n",
      "Step 50 - Loss: 97.1957\n",
      "Step 60 - Loss: 110.9271\n",
      "Step 70 - Loss: 110.1096\n",
      "Epoch 1 - MSE: 632.493828125, MAE: 19.49561309814453\n",
      "\n",
      "Epoch 2/10\n",
      "Step 0 - Loss: 93.3881\n",
      "Step 10 - Loss: 114.5573\n",
      "Step 20 - Loss: 94.0496\n",
      "Step 30 - Loss: 93.6874\n",
      "Step 40 - Loss: 107.0565\n",
      "Step 50 - Loss: 96.8008\n",
      "Step 60 - Loss: 110.8149\n",
      "Step 70 - Loss: 109.8749\n",
      "Epoch 2 - MSE: 665.335859375, MAE: 19.201513671875\n",
      "\n",
      "Epoch 3/10\n",
      "Step 0 - Loss: 93.3804\n",
      "Step 10 - Loss: 114.5897\n",
      "Step 20 - Loss: 94.0521\n",
      "Step 30 - Loss: 93.7049\n",
      "Step 40 - Loss: 107.1218\n",
      "Step 50 - Loss: 96.9099\n",
      "Step 60 - Loss: 110.8794\n",
      "Step 70 - Loss: 110.1371\n",
      "Epoch 3 - MSE: 688.03234375, MAE: 19.402401733398438\n",
      "\n",
      "Epoch 4/10\n",
      "Step 0 - Loss: 93.4093\n",
      "Step 10 - Loss: 114.4519\n",
      "Step 20 - Loss: 94.0513\n",
      "Step 30 - Loss: 93.6781\n",
      "Step 40 - Loss: 107.0361\n",
      "Step 50 - Loss: 96.8038\n",
      "Step 60 - Loss: 110.8105\n",
      "Step 70 - Loss: 109.8761\n",
      "Epoch 4 - MSE: 663.79734375, MAE: 19.119477844238283\n",
      "\n",
      "Epoch 5/10\n",
      "Step 0 - Loss: 93.4052\n",
      "Step 10 - Loss: 114.5856\n",
      "Step 20 - Loss: 94.0550\n",
      "Step 30 - Loss: 93.6657\n",
      "Step 40 - Loss: 107.0890\n",
      "Step 50 - Loss: 96.8581\n",
      "Step 60 - Loss: 110.8223\n",
      "Step 70 - Loss: 109.8895\n",
      "Epoch 5 - MSE: 626.5334765625, MAE: 18.751589965820312\n",
      "\n",
      "Epoch 6/10\n",
      "Step 0 - Loss: 93.3771\n",
      "Step 10 - Loss: 114.5832\n",
      "Step 20 - Loss: 94.0705\n",
      "Step 30 - Loss: 93.6534\n",
      "Step 40 - Loss: 107.0599\n",
      "Step 50 - Loss: 96.7507\n",
      "Step 60 - Loss: 110.8796\n",
      "Step 70 - Loss: 109.9266\n",
      "Epoch 6 - MSE: 690.169609375, MAE: 19.400045776367186\n",
      "\n",
      "Epoch 7/10\n",
      "Step 0 - Loss: 93.4248\n",
      "Step 10 - Loss: 114.4453\n",
      "Step 20 - Loss: 94.0505\n",
      "Step 30 - Loss: 93.6431\n",
      "Step 40 - Loss: 107.0124\n",
      "Step 50 - Loss: 96.7423\n",
      "Step 60 - Loss: 110.9312\n",
      "Step 70 - Loss: 110.0339\n",
      "Epoch 7 - MSE: 691.608125, MAE: 19.415576171875\n",
      "\n",
      "Epoch 8/10\n",
      "Step 0 - Loss: 93.4255\n",
      "Step 10 - Loss: 114.4249\n",
      "Step 20 - Loss: 94.0432\n",
      "Step 30 - Loss: 93.6500\n",
      "Step 40 - Loss: 107.0542\n",
      "Step 50 - Loss: 96.9326\n",
      "Step 60 - Loss: 110.8959\n",
      "Step 70 - Loss: 109.6930\n",
      "Epoch 8 - MSE: 903.5646875, MAE: 22.683319091796875\n",
      "\n",
      "Epoch 9/10\n",
      "Step 0 - Loss: 93.4743\n",
      "Step 10 - Loss: 114.4796\n",
      "Step 20 - Loss: 94.0407\n",
      "Step 30 - Loss: 93.6732\n",
      "Step 40 - Loss: 107.0836\n",
      "Step 50 - Loss: 96.7242\n",
      "Step 60 - Loss: 110.8405\n",
      "Step 70 - Loss: 110.2011\n",
      "Epoch 9 - MSE: 581.2471484375, MAE: 18.18432159423828\n",
      "\n",
      "Epoch 10/10\n",
      "Step 0 - Loss: 93.3497\n",
      "Step 10 - Loss: 114.5011\n",
      "Step 20 - Loss: 94.0656\n",
      "Step 30 - Loss: 93.6269\n",
      "Step 40 - Loss: 107.0030\n",
      "Step 50 - Loss: 96.7206\n",
      "Step 60 - Loss: 110.8448\n",
      "Step 70 - Loss: 110.0258\n",
      "Epoch 10 - MSE: 654.5660546875, MAE: 18.899661254882812\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths for dataset\n",
    "TRAIN_IMAGES_DIR = \"C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\images\"\n",
    "TRAIN_ANNOTATIONS_DIR = \"C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\ground_truth_npy\"\n",
    "TEST_IMAGES_DIR = \"C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\test_data\\\\images\"\n",
    "TEST_ANNOTATIONS_DIR = \"C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\test_data\\\\ground_truth_npy\"\n",
    "\n",
    "# Data loader functions\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "def load_annotation(annotation_path):\n",
    "    annotation = np.load(annotation_path.numpy().decode(\"utf-8\"))\n",
    "    return annotation.astype(np.float32)\n",
    "\n",
    "def process_data(img_path, ann_path, img_size):\n",
    "    img = load_image(img_path)\n",
    "    ann = tf.py_function(load_annotation, [ann_path], tf.float32)\n",
    "    ann.set_shape([None, None])\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    ann = tf.image.resize(tf.expand_dims(ann, -1), img_size)\n",
    "    return img, ann\n",
    "\n",
    "def prepare_datasets(batch_size=4, img_size=(256, 256)):\n",
    "    train_images = sorted(glob.glob(f\"{TRAIN_IMAGES_DIR}/*.jpg\"))\n",
    "    train_annotations = sorted(glob.glob(f\"{TRAIN_ANNOTATIONS_DIR}/*.npy\"))\n",
    "\n",
    "    test_images = sorted(glob.glob(f\"{TEST_IMAGES_DIR}/*.jpg\"))\n",
    "    test_annotations = sorted(glob.glob(f\"{TEST_ANNOTATIONS_DIR}/*.npy\"))\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((train_images, train_annotations))\n",
    "    train_data = train_data.map(lambda x, y: process_data(x, y, img_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_data = train_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((test_images, test_annotations))\n",
    "    test_data = test_data.map(lambda x, y: process_data(x, y, img_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_data = test_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Define VGG model with custom absolute layer\n",
    "class AbsoluteLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.abs(inputs)\n",
    "\n",
    "def vgg19(input_shape=(256, 256, 3)):\n",
    "    config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for v in config:\n",
    "        if v == 'M':\n",
    "            x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "        else:\n",
    "            x = layers.Conv2D(v, kernel_size=(3, 3), padding='same')(x)\n",
    "            x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(256, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(128, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(1, kernel_size=(1, 1))(x)\n",
    "    x = AbsoluteLayer()(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# CHF Loss\n",
    "class CHFLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, chf_step=30, chf_tik=0.01):\n",
    "        super(CHFLoss, self).__init__()\n",
    "        self.chf_step = chf_step\n",
    "        self.chf_tik = chf_tik\n",
    "\n",
    "    def chf_calculation(self, y_flat):\n",
    "        freqs = tf.range(-self.chf_step, self.chf_step, dtype=tf.float32) * self.chf_tik\n",
    "        freq_grid = tf.reshape(freqs, [1, -1])\n",
    "        y_flat_expanded = tf.expand_dims(y_flat, -1)\n",
    "        chf_real = tf.reduce_sum(tf.cos(tf.matmul(y_flat_expanded, freq_grid)), axis=1)\n",
    "        chf_img = tf.reduce_sum(tf.sin(tf.matmul(y_flat_expanded, freq_grid)), axis=1)\n",
    "        return chf_real, chf_img\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true_flat = tf.reshape(y_true, [tf.shape(y_true)[0], -1])\n",
    "        y_pred_flat = tf.reshape(y_pred, [tf.shape(y_pred)[0], -1])\n",
    "        true_chf_real, true_chf_img = self.chf_calculation(y_true_flat)\n",
    "        pred_chf_real, pred_chf_img = self.chf_calculation(y_pred_flat)\n",
    "        real_loss = tf.reduce_mean(tf.square(true_chf_real - pred_chf_real))\n",
    "        img_loss = tf.reduce_mean(tf.square(true_chf_img - pred_chf_img))\n",
    "        return (real_loss + img_loss) / 1_000_000  # Scale loss\n",
    "\n",
    "# Training function with scaling for losses and metrics\n",
    "def train_model(model, train_data, test_data, chf_loss, optimizer, epochs=10):\n",
    "    mse_metric = tf.keras.metrics.MeanSquaredError()\n",
    "    mae_metric = tf.keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        for step, (x_batch, y_batch) in enumerate(train_data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(x_batch, training=True)\n",
    "                loss = chf_loss(y_batch, predictions)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Step {step} - Loss: {loss:.4f}\")\n",
    "\n",
    "        mse_metric.reset_state()\n",
    "        mae_metric.reset_state()\n",
    "        for x_batch_test, y_batch_test in test_data:\n",
    "            test_predictions = model(x_batch_test, training=False)\n",
    "            test_predictions_resized = tf.image.resize(test_predictions, [256, 256])\n",
    "\n",
    "            mse_metric.update_state(y_batch_test, test_predictions_resized)\n",
    "            mae_metric.update_state(y_batch_test, test_predictions_resized)\n",
    "\n",
    "        # Print scaled metrics\n",
    "        mse_scaled = mse_metric.result().numpy() / 100\n",
    "        mae_scaled = mae_metric.result().numpy() / 10\n",
    "        print(f\"Epoch {epoch + 1} - MSE: {mse_scaled}, MAE: {mae_scaled}\")\n",
    "\n",
    "# Visualization function for density maps\n",
    "def visualize_predictions(model, test_data, save_dir='predictions'):\n",
    "    import os\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    for i, (x_batch_test, _) in enumerate(test_data.take(5)):  # visualize 5 test samples\n",
    "        pred_density_map = model(x_batch_test, training=False)\n",
    "        pred_density_map_resized = tf.image.resize(pred_density_map, [256, 256]).numpy()\n",
    "\n",
    "        for j, density_map in enumerate(pred_density_map_resized):\n",
    "            plt.imshow(density_map.squeeze(), cmap='jet')\n",
    "            plt.colorbar()\n",
    "            plt.title(f\"Predicted Density Map {i}_{j}\")\n",
    "            plt.savefig(f\"{save_dir}/density_map_{i}_{j}.png\")\n",
    "            plt.close()\n",
    "\n",
    "# Initialize datasets, model, CHF loss, and optimizer\n",
    "train_data, test_data = prepare_datasets(batch_size=4, img_size=(256, 256))\n",
    "model = vgg19()\n",
    "chf_loss = CHFLoss(chf_step=30, chf_tik=0.01)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_data, test_data, chf_loss, optimizer, epochs=10)\n",
    "\n",
    "# Visualize predictions\n",
    "visualize_predictions(model, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4569bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Step 0 - Loss: 102.2162\n",
      "Step 10 - Loss: 96.8978\n",
      "Step 20 - Loss: 108.1253\n",
      "Step 30 - Loss: 105.9141\n",
      "\n",
      "Epoch 2/10\n",
      "Step 0 - Loss: 102.2069\n",
      "Step 10 - Loss: 96.6600\n",
      "Step 20 - Loss: 107.6529\n",
      "Step 30 - Loss: 105.5431\n",
      "MSE: 726.014609375, MAE: 19.927342224121094\n",
      "\n",
      "Epoch 3/10\n",
      "Step 0 - Loss: 101.7451\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13700\\3544601040.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrowdCountingTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchf_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13700\\3544601040.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nEpoch {epoch + 1}/{self.epochs}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Save model and evaluate every 2 epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13700\\3544601040.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchf_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m1e6\u001b[0m  \u001b[1;31m# Scale for readability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Step {step} - Loss: {loss:.4f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m         \u001b[1;31m# Return iterations for compat with tf.keras.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;31m# Apply gradient updates.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend_apply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m             \u001b[1;31m# Apply variable constraints after applying gradients.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\u001b[0m in \u001b[0;36m_backend_apply_gradients\u001b[1;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[1;31m# Run udpate step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m             self._backend_update_step(\n\u001b[0m\u001b[0;32m    473\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py\u001b[0m in \u001b[0;36m_backend_update_step\u001b[1;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_all_reduce_sum_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[0m\u001b[0;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distributed_tf_update_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py\u001b[0m in \u001b[0;36mmaybe_merge_call\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m   \"\"\"\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     return distribute_lib.get_replica_context().merge_call(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py\u001b[0m in \u001b[0;36m_distributed_tf_update_step\u001b[1;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             distribution.extended.update(\n\u001b[0m\u001b[0;32m    137\u001b[0m                 \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3003\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m   3004\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3005\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3006\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3007\u001b[0m       return self._replica_ctx_update(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   4073\u001b[0m     \u001b[1;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4074\u001b[0m     \u001b[1;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4075\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4076\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4077\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   4079\u001b[0m     \u001b[1;31m# once that value is used for something.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4080\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4081\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4082\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4083\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[1;34m(var, grad, learning_rate)\u001b[0m\n\u001b[0;32m    131\u001b[0m     ):\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\adam.py\u001b[0m in \u001b[0;36mupdate_step\u001b[1;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             ops.multiply(\n\u001b[1;32m--> 138\u001b[1;33m                 \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m             ),\n\u001b[0;32m    140\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\ops\\numpy.py\u001b[0m in \u001b[0;36msubtract\u001b[1;34m(x1, x2)\u001b[0m\n\u001b[0;32m   5801\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5802\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mSubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymbolic_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5803\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5804\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\sparse.py\u001b[0m in \u001b[0;36msparse_wrapper\u001b[1;34m(x1, x2)\u001b[0m\n\u001b[0;32m    491\u001b[0m                 \u001b[1;31m# x2 is an IndexedSlices, densify.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m                 \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msparse_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\u001b[0m in \u001b[0;36msubtract\u001b[1;34m(x1, x2)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1260\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1261\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36msubtract\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m  13680\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13681\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 13682\u001b[1;33m       return sub_eager_fallback(\n\u001b[0m\u001b[0;32m  13683\u001b[0m           x, y, name=name, ctx=_ctx)\n\u001b[0;32m  13684\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36msub_eager_fallback\u001b[1;34m(x, y, name, ctx)\u001b[0m\n\u001b[0;32m  13704\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13705\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"T\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 13706\u001b[1;33m   _result = _execute.execute(b\"Sub\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0m\u001b[0;32m  13707\u001b[0m                              ctx=ctx, name=name)\n\u001b[0;32m  13708\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths for dataset\n",
    "TRAIN_IMAGES_DIR = \"C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\images\"\n",
    "TRAIN_ANNOTATIONS_DIR = \"C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\train_data\\\\ground_truth_npy\"\n",
    "TEST_IMAGES_DIR = \"C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\test_data\\\\images\"\n",
    "TEST_ANNOTATIONS_DIR = \"C:\\\\Users\\\\hp\\\\Downloads\\\\ShanghaiTech_Crowd_Counting_Dataset\\\\part_A_final\\\\test_data\\\\ground_truth_npy\"\n",
    "\n",
    "# Data loader functions\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "def load_annotation(annotation_path):\n",
    "    annotation = np.load(annotation_path.numpy().decode(\"utf-8\"))\n",
    "    return annotation.astype(np.float32)\n",
    "\n",
    "def process_data(img_path, ann_path, img_size):\n",
    "    img = load_image(img_path)\n",
    "    ann = tf.py_function(load_annotation, [ann_path], tf.float32)\n",
    "    ann.set_shape([None, None])\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    ann = tf.image.resize(tf.expand_dims(ann, -1), img_size)\n",
    "    return img, ann\n",
    "\n",
    "def prepare_datasets(batch_size=8, img_size=(256, 256)):\n",
    "    train_images = sorted(glob.glob(f\"{TRAIN_IMAGES_DIR}/*.jpg\"))\n",
    "    train_annotations = sorted(glob.glob(f\"{TRAIN_ANNOTATIONS_DIR}/*.npy\"))\n",
    "    test_images = sorted(glob.glob(f\"{TEST_IMAGES_DIR}/*.jpg\"))\n",
    "    test_annotations = sorted(glob.glob(f\"{TEST_ANNOTATIONS_DIR}/*.npy\"))\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((train_images, train_annotations))\n",
    "    train_data = train_data.map(lambda x, y: process_data(x, y, img_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_data = train_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((test_images, test_annotations))\n",
    "    test_data = test_data.map(lambda x, y: process_data(x, y, img_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_data = test_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Define VGG model with modified regression head\n",
    "class AbsoluteLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.abs(inputs)\n",
    "\n",
    "def vgg19(input_shape=(256, 256, 3)):\n",
    "    config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for v in config:\n",
    "        if v == 'M':\n",
    "            x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "        else:\n",
    "            x = layers.Conv2D(v, kernel_size=(3, 3), padding='same')(x)\n",
    "            x = layers.ReLU()(x)\n",
    "    # Custom regression head\n",
    "    x = layers.Conv2D(256, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(128, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(1, kernel_size=(1, 1))(x)\n",
    "    x = AbsoluteLayer()(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# CHF Loss with fine-tuned parameters\n",
    "class CHFLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, chf_step=30, chf_tik=0.01, integral_range=0.3, grid_granularity=0.01):\n",
    "        super(CHFLoss, self).__init__()\n",
    "        self.chf_step = chf_step\n",
    "        self.chf_tik = chf_tik\n",
    "        self.integral_range = integral_range\n",
    "        self.grid_granularity = grid_granularity\n",
    "\n",
    "    def chf_calculation(self, y_flat):\n",
    "        freqs = tf.range(-self.chf_step, self.chf_step, dtype=tf.float32) * self.chf_tik\n",
    "        freq_grid = tf.reshape(freqs, [1, -1])\n",
    "        y_flat_expanded = tf.expand_dims(y_flat, -1)\n",
    "        chf_real = tf.reduce_sum(tf.cos(tf.matmul(y_flat_expanded, freq_grid)), axis=1)\n",
    "        chf_img = tf.reduce_sum(tf.sin(tf.matmul(y_flat_expanded, freq_grid)), axis=1)\n",
    "        return chf_real, chf_img\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true_flat = tf.reshape(y_true, [tf.shape(y_true)[0], -1])\n",
    "        y_pred_flat = tf.reshape(y_pred, [tf.shape(y_pred)[0], -1])\n",
    "        true_chf_real, true_chf_img = self.chf_calculation(y_true_flat)\n",
    "        pred_chf_real, pred_chf_img = self.chf_calculation(y_pred_flat)\n",
    "        real_loss = tf.reduce_mean(tf.square(true_chf_real - pred_chf_real))\n",
    "        img_loss = tf.reduce_mean(tf.square(true_chf_img - pred_chf_img))\n",
    "        return real_loss + img_loss\n",
    "\n",
    "# Trainer Class with updated evaluation\n",
    "class CrowdCountingTrainer:\n",
    "    def __init__(self, model, train_data, test_data, optimizer, chf_loss, epochs=10):\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.optimizer = optimizer\n",
    "        self.chf_loss = chf_loss\n",
    "        self.epochs = epochs\n",
    "        self.mse_metric = tf.keras.metrics.MeanSquaredError()\n",
    "        self.mae_metric = tf.keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        for step, (x_batch, y_batch) in enumerate(self.train_data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self.model(x_batch, training=True)\n",
    "                loss = self.chf_loss(y_batch, predictions) / 1e6  # Scale for readability\n",
    "            grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Step {step} - Loss: {loss:.4f}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.mse_metric.reset_state()\n",
    "        self.mae_metric.reset_state()\n",
    "        for x_batch_test, y_batch_test in self.test_data:\n",
    "            test_predictions = self.model(x_batch_test, training=False)\n",
    "            test_predictions_resized = tf.image.resize(test_predictions, [256, 256])\n",
    "            self.mse_metric.update_state(y_batch_test, test_predictions_resized)\n",
    "            self.mae_metric.update_state(y_batch_test, test_predictions_resized)\n",
    "        print(f\"MSE: {self.mse_metric.result().numpy() }, MAE: {self.mae_metric.result().numpy() }\")\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{self.epochs}\")\n",
    "            self.train_epoch()\n",
    "            if (epoch + 1) % 2 == 0:  # Save model and evaluate every 2 epochs\n",
    "                self.evaluate()\n",
    "\n",
    "# Model Training and Evaluation\n",
    "train_data, test_data = prepare_datasets(batch_size=8, img_size=(256, 256))\n",
    "model = vgg19()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "chf_loss = CHFLoss(chf_step=30, chf_tik=0.01, integral_range=0.3, grid_granularity=0.01)\n",
    "\n",
    "trainer = CrowdCountingTrainer(model, train_data, test_data, optimizer, chf_loss, epochs=10)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d2bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e071b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
